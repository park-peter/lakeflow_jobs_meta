{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakeflow Job Meta - Orchestrator Example\n",
    "\n",
    "This notebook demonstrates how to use the Lakeflow Job Meta framework to create and manage metadata-driven Databricks Lakeflow Jobs.\n",
    "\n",
    "## Features\n",
    "- Supports multiple task types: Notebook, SQL Query, SQL File\n",
    "- Dynamic job generation from metadata\n",
    "- Job lifecycle management (create/update/track)\n",
    "- Execution order and dependency management\n",
    "- Optional continuous monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33ebe32-8c36-4b18-88a7-acf54cf0fa8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import framework modules\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Dynamically detect project root from notebook location\n",
    "try:\n",
    "    # Get notebook path from Databricks context\n",
    "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    # Extract project root directory (go up from examples/)\n",
    "    project_root = os.path.dirname(os.path.dirname(notebook_path))\n",
    "    sys.path.insert(0, project_root)\n",
    "    logger_tmp = logging.getLogger(__name__)\n",
    "    logger_tmp.info(f\"✅ Added project root to path: {project_root}\")\n",
    "except Exception as e:\n",
    "    # Fallback: Try current directory\n",
    "    current_dir = os.path.abspath('.')\n",
    "    if os.path.exists(os.path.join(current_dir, 'lakeflow_job_meta')):\n",
    "        sys.path.insert(0, current_dir)\n",
    "    else:\n",
    "        # If package is installed, this is fine\n",
    "        pass\n",
    "\n",
    "# Import framework\n",
    "from lakeflow_job_meta import JobOrchestrator, MetadataManager, MetadataMonitor\n",
    "from lakeflow_job_meta.constants import SUPPORTED_TASK_TYPES\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Supported task types: {', '.join(SUPPORTED_TASK_TYPES)}\")\n",
    "\n",
    "# MAGIC %md\n",
    "# ## Configuration Widgets\n",
    "# \n",
    "# Configure parameters using Databricks widgets. These can also be set via base_parameters when running as a job.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create widgets for configuration\n",
    "dbutils.widgets.text(\"control_table\", \"\", \"Control Table (required)\")\n",
    "dbutils.widgets.text(\"yaml_path\", \"\", \"YAML Path (optional)\")\n",
    "dbutils.widgets.text(\"volume_path\", \"\", \"Volume Path (optional)\")\n",
    "dbutils.widgets.text(\"sync_yaml\", \"false\", \"Sync YAML (true/false)\")\n",
    "\n",
    "# Get widget values\n",
    "CONTROL_TABLE = dbutils.widgets.get(\"control_table\")\n",
    "YAML_PATH = dbutils.widgets.get(\"yaml_path\") or None\n",
    "VOLUME_PATH = dbutils.widgets.get(\"volume_path\") or None\n",
    "SYNC_YAML = dbutils.widgets.get(\"sync_yaml\").lower() == \"true\"\n",
    "\n",
    "# Validate required parameter\n",
    "if not CONTROL_TABLE:\n",
    "    raise ValueError(\"control_table widget is required. Please set it in the widget or via base_parameters.\")\n",
    "\n",
    "logger.info(f\"Configuration:\")\n",
    "logger.info(f\"  Control Table: {CONTROL_TABLE}\")\n",
    "logger.info(f\"  YAML Path: {YAML_PATH or 'Not configured'}\")\n",
    "logger.info(f\"  Volume Path: {VOLUME_PATH or 'Not configured'}\")\n",
    "logger.info(f\"  Sync YAML: {SYNC_YAML}\")\n",
    "\n",
    "# Initialize MetadataManager and JobOrchestrator\n",
    "metadata_manager = MetadataManager(CONTROL_TABLE)\n",
    "orchestrator = JobOrchestrator(CONTROL_TABLE)\n",
    "\n",
    "# Ensure control table exists\n",
    "metadata_manager.ensure_exists()\n",
    "\n",
    "# Optionally load YAML if file exists and SYNC_YAML is enabled\n",
    "if SYNC_YAML and YAML_PATH and os.path.exists(YAML_PATH):\n",
    "    try:\n",
    "        sources_loaded = metadata_manager.load_yaml(YAML_PATH)\n",
    "        logger.info(f\"✅ Loaded {sources_loaded} sources from YAML: {YAML_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"⚠️ Failed to load YAML: {str(e)}. Using existing table data.\")\n",
    "elif SYNC_YAML and YAML_PATH and not os.path.exists(YAML_PATH):\n",
    "    logger.info(f\"ℹ️ YAML file not found at {YAML_PATH}. Using existing table data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbfba10-1ded-42e1-b5f7-89df0571e86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the orchestrator\n",
    "# This will read from the control table and create/update jobs for all modules\n",
    "\n",
    "jobs = orchestrator.run_all_modules(\n",
    "    auto_run=True,  # Automatically run jobs after creation\n",
    "    yaml_path=YAML_PATH if SYNC_YAML else None,\n",
    "    sync_yaml=False  # Already synced above if needed\n",
    ")\n",
    "\n",
    "logger.info(f\"✅ Managed {len(jobs)} jobs successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Continuous Monitoring Mode\n",
    "\n",
    "If you want to continuously monitor for metadata changes, uncomment and run the cell below.\n",
    "\n",
    "This will:\n",
    "- Watch the control table for changes\n",
    "- Optionally sync YAML files from a Unity Catalog volume\n",
    "- Automatically update jobs when metadata changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to enable continuous monitoring\n",
    "# This will check for metadata changes every 60 seconds and auto-update jobs\n",
    "# The CONTROL_TABLE and VOLUME_PATH are taken from widgets configured above\n",
    "\n",
    "# monitor = MetadataMonitor(\n",
    "#     control_table=CONTROL_TABLE,\n",
    "#     check_interval_seconds=60,\n",
    "#     volume_path=VOLUME_PATH,  # Optional: watch Unity Catalog volume for YAML files\n",
    "#     auto_update_jobs=True\n",
    "# )\n",
    "# monitor.run_continuous(max_iterations=None)  # None = run indefinitely\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2013334455468117,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "custom_metadata_sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
