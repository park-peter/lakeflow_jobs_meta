# Example Metadata Configurations for Lakeflow Jobs Meta Framework v0.2.0
# This file demonstrates different task types and use cases
#
# ⚠️  IMPORTANT: Update these examples for your environment!
#     - Replace workspace paths (/Workspace/Users/...) with your own paths
#     - Replace catalog names (fe_ppark_demo) with your catalog
#     - Replace cluster IDs with your cluster IDs
#     - Replace warehouse IDs with your warehouse IDs
#     - Update email addresses in notification settings
#
# New in v0.2.0:
#   - Dict-based YAML format: jobs are now dict keys (resource_ids)
#   - Optional 'name' field: if provided, used as Databricks job name; otherwise resource_id is used
#   - Variable substitution: use ${var.variable_name} for dynamic values
#
# Job-Level Settings (optional):
#   - edit_mode: "UI_LOCKED" (default) or "EDITABLE"
#     Controls whether jobs can be edited in Databricks UI
#     UI_LOCKED: Recommended for production (prevents accidental edits)
#     EDITABLE: Allows UI editing (useful for experimental jobs)

metadata_version: "2.0"

jobs:
  # Example 1: SQL Query Task with Job-Level and Task-Level Settings
  data_quality_checks:
    description: "Simple data quality validation using inline SQL query (no table dependencies)"
    timeout_seconds: 7200
    max_concurrent_runs: 1
    queue:
      enabled: true
    continuous:
      pause_status: PAUSED
      task_retry_mode: ON_FAILURE
    tasks:
      - task_key: "simple_quality_check"
        task_type: "sql_query"
        disabled: true  # Task starts as disabled
        timeout_seconds: 3600  # Task-level timeout
        sql_query: |
          SELECT 
            CURRENT_TIMESTAMP() as check_timestamp,
            'data_quality_check' as check_name,
            CAST(5.0 AS DOUBLE) as threshold_value,
            CASE 
              WHEN CAST(5.0 AS DOUBLE) <= 5.0 THEN 'PASS'
              ELSE 'FAIL'
            END as quality_status,
            'Example validation check' as description

  # Example 2: SQL File Tasks
  sales_pipeline:
    description: "Create sample data, then transform and aggregate using SQL files"
    tasks:
      - task_key: "create_sample_data"
        task_type: "sql_file"
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/01_create_sample_data.sql"
        parameters:
          catalog: "${var.catalog}"
          schema: "lakeflow_jobs_metadata"
      
      - task_key: "transform_to_silver"
        task_type: "sql_file"
        depends_on: ["create_sample_data"]
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/03_bronze_to_silver_transformation.sql"
        parameters:
          catalog: "${var.catalog}"
          schema: "lakeflow_jobs_metadata"
      
      - task_key: "daily_aggregations"
        task_type: "sql_file"
        depends_on: ["transform_to_silver"]
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/sql_file_task/02_daily_aggregations.sql"
        parameters:
          catalog: "${var.catalog}"
          schema: "lakeflow_jobs_metadata"

  # Example 3: Mixed Task Types
  ingestion_and_validation:
    description: "Ingest data via notebook, then validate with SQL"
    tasks:
      - task_key: "delta_table_ingestion"
        task_type: "notebook"
        file_path: "/Workspace/Users/peter.park@databricks.com/lakeflow_jobs_meta/examples/notebook_task/sample_ingestion_notebook"
        parameters:
          catalog: "${var.catalog}"
          schema: "lakeflow_jobs_metadata"
          source_table: "source_customers"
          target_table: "customers"
          write_mode: "append"
      
      - task_key: "validate_data"
        task_type: "sql_query"
        depends_on: ["delta_table_ingestion"]
        sql_query: |
          SELECT 
            CURRENT_TIMESTAMP() as validation_timestamp,
            'data_validation' as validation_type,
            'SUCCESS' as status,
            'Ingestion completed successfully' as message

  # Example 4: Using 'name' field (resource_id != job_name)
  reporting_pipeline:
    name: "Daily Reporting Queries"  # This will be the job name in Databricks
    description: "Use pre-saved SQL queries from Databricks SQL"
    tasks:
      - task_key: "revenue_report"
        task_type: "sql_query"
        query_id: "911c517b-d93e-4ab1-b065-ce361df23f8b"
        parameters:
          threshold: "5.0"

  # Example 5: Scheduled Job
  daily_etl:
    description: "Daily ETL pipeline with schedule"
    schedule:
      quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
      timezone_id: "America/Los_Angeles"
      pause_status: UNPAUSED
    tasks:
      - task_key: "extract_data"
        task_type: "notebook"
        file_path: "/Workspace/Users/peter.park@databricks.com/notebooks/extract"

  # Example 6: Job with File Arrival Trigger
  file_arrival_job:
    description: "Process files as they arrive"
    trigger:
      pause_status: UNPAUSED
      file_arrival:
        url: "/Volumes/fe_ppark_demo/lakeflow_jobs_metadata/incoming/"
    tasks:
      - task_key: "process_file"
        task_type: "notebook"
        file_path: "/Workspace/Users/peter.park@databricks.com/notebooks/process"

  # Example 7: Advanced Job with Multiple Features
  advanced_job_example:
    name: "Advanced Multi-Task Pipeline"
    description: "Comprehensive example with job clusters, environments, and notifications"
    parameters:
      - default: "${var.catalog}"
        name: "default_catalog"
      - default: "lakeflow_jobs_metadata"
        name: "default_schema"
    tags:
      department: "engineering"
      project: "lakeflow_demo"
    
    # Job-level clusters
    job_clusters:
      - job_cluster_key: "Job_cluster_aws"
        new_cluster:
          spark_version: "16.4.x-scala2.12"
          node_type_id: "rd-fleet.xlarge"
          num_workers: 8
          aws_attributes:
            availability: SPOT_WITH_FALLBACK
            ebs_volume_type: GENERAL_PURPOSE_SSD
            ebs_volume_count: 1
            ebs_volume_size: 100
          data_security_mode: SINGLE_USER
          runtime_engine: PHOTON
      
      # - job_cluster_key: "Job_cluster_azure"
      #   new_cluster:
      #     spark_version: "16.4.x-scala2.12"
      #     node_type_id: "Standard_DS3_v2"
      #     num_workers: 4
      #     azure_attributes:
      #       availability: SPOT_WITH_FALLBACK_AZURE
      #       first_on_demand: 1
      #     data_security_mode: SINGLE_USER
      
      # - job_cluster_key: "Job_cluster_gcp"
      #   new_cluster:
      #     spark_version: "16.4.x-scala2.12"
      #     node_type_id: "n1-standard-4"
      #     num_workers: 4
      #     gcp_attributes:
      #       availability: PREEMPTIBLE_WITH_FALLBACK_GCP
      #       boot_disk_size: 100
      #     data_security_mode: SINGLE_USER
    
    # Notification settings
    notification_settings:
      email_notifications:
        on_failure: ["alerts@example.com"]
        on_duration_warning_threshold_exceeded: ["alerts@example.com"]
      alert_on_last_attempt: true
    
    tasks:
      - task_key: "task_with_job_cluster_aws"
        task_type: "notebook"
        job_cluster_key: "Job_cluster_aws"
        file_path: "/Workspace/Users/peter.park@databricks.com/notebooks/sample"
      
      # - task_key: "task_with_job_cluster_azure"
      #   task_type: "notebook"
      #   job_cluster_key: "Job_cluster_azure"
      #   file_path: "/Workspace/Users/peter.park@databricks.com/notebooks/sample"
      
      # - task_key: "task_with_job_cluster_gcp"
      #   task_type: "notebook"
      #   job_cluster_key: "Job_cluster_gcp"
      #   file_path: "/Workspace/Users/peter.park@databricks.com/notebooks/sample"
      
      - task_key: "task_with_run_if"
        task_type: "sql_query"
        depends_on: [
          "task_with_job_cluster_aws", 
          # "task_with_job_cluster_azure", 
          # "task_with_job_cluster_gcp"
          ]
        run_if: "AT_LEAST_ONE_SUCCESS"
        sql_query: "SELECT 'Cleanup completed' as status"
