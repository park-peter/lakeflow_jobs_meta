{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Framework Ingestion Notebook Example\n",
        "\n",
        "This notebook demonstrates the framework contract for ingestion tasks.\n",
        "It reads from a source Delta table, transforms the data, and writes to a target Delta table.\n",
        "\n",
        "**Framework Contract:**\n",
        "- Accepts `source_id` and `control_table` as inputs via widgets\n",
        "- Reads catalog, schema, and table names from the control table metadata using source_id\n",
        "- In production, the framework will pass source_id and control_table via widgets\n",
        "- For this example, widgets have default values so it can run end-to-end without manual input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import json\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Framework widgets - In production, these are set by the framework\n",
        "# For this example, widgets have default values so it can run end-to-end\n",
        "dbutils.widgets.text(\"source_id\", \"delta_table_ingestion\", \"Source ID\")\n",
        "dbutils.widgets.text(\"control_table\", \"main.examples.etl_control\", \"Control Table\")\n",
        "\n",
        "# Get widget values\n",
        "source_id = dbutils.widgets.get(\"source_id\")\n",
        "control_table = dbutils.widgets.get(\"control_table\")\n",
        "\n",
        "if not source_id:\n",
        "    raise ValueError(\"source_id widget is required\")\n",
        "if not control_table:\n",
        "    raise ValueError(\"control_table widget is required\")\n",
        "\n",
        "logger.info(f\"Reading metadata for source_id: {source_id} from control table: {control_table}\")\n",
        "\n",
        "# Read metadata from control table\n",
        "try:\n",
        "    metadata_df = spark.table(control_table).filter(\n",
        "        (F.col(\"source_id\") == source_id) & (F.col(\"is_active\") == True)\n",
        "    ).select(\"source_config\", \"target_config\").first()\n",
        "    \n",
        "    if not metadata_df:\n",
        "        raise ValueError(f\"No active metadata found for source_id '{source_id}' in control table '{control_table}'\")\n",
        "    \n",
        "    # Parse JSON configs\n",
        "    source_config_str = metadata_df['source_config']\n",
        "    target_config_str = metadata_df['target_config']\n",
        "    \n",
        "    source_config = json.loads(source_config_str) if isinstance(source_config_str, str) else source_config_str\n",
        "    target_config = json.loads(target_config_str) if isinstance(target_config_str, str) else target_config_str\n",
        "    \n",
        "    # Extract catalog, schema, and table names\n",
        "    catalog = source_config.get('catalog') or target_config.get('catalog')\n",
        "    schema = source_config.get('schema') or target_config.get('schema')\n",
        "    source_table = source_config.get('table')\n",
        "    target_table = target_config.get('table')\n",
        "    write_mode = target_config.get('write_mode', 'overwrite')\n",
        "    \n",
        "    # Validate required fields\n",
        "    if not catalog:\n",
        "        raise ValueError(f\"Missing 'catalog' in source_config or target_config for source_id '{source_id}'\")\n",
        "    if not schema:\n",
        "        raise ValueError(f\"Missing 'schema' in source_config or target_config for source_id '{source_id}'\")\n",
        "    if not source_table:\n",
        "        raise ValueError(f\"Missing 'table' in source_config for source_id '{source_id}'\")\n",
        "    if not target_table:\n",
        "        raise ValueError(f\"Missing 'table' in target_config for source_id '{source_id}'\")\n",
        "    \n",
        "    logger.info(f\"Successfully read metadata from control table\")\n",
        "    logger.info(f\"Catalog: {catalog}, Schema: {schema}\")\n",
        "    logger.info(f\"Source table: {source_table}\")\n",
        "    logger.info(f\"Target table: {target_table}\")\n",
        "    logger.info(f\"Write mode: {write_mode}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to read metadata from control table: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse and Validate Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration already validated when reading from control table\n",
        "logger.info(\"Configuration validated successfully\")\n",
        "logger.info(f\"Source: {catalog}.{schema}.{source_table}\")\n",
        "logger.info(f\"Target: {catalog}.{schema}.{target_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Source Data\n",
        "\n",
        "If the source table doesn't exist, create sample data for demonstration.\n",
        "In production, this would read directly from the configured source table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "source_table_full = f\"{catalog}.{schema}.{source_table}\"\n",
        "\n",
        "logger.info(f\"Reading from source table: {source_table_full}\")\n",
        "\n",
        "# Try to read from source table, if it doesn't exist, create sample data\n",
        "df = None\n",
        "record_count = 0\n",
        "\n",
        "try:\n",
        "    df = spark.table(source_table_full)\n",
        "    record_count = df.count()\n",
        "    logger.info(f\"Successfully read {record_count} records from source table\")\n",
        "    df.show(5, truncate=False)\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Source table not found: {source_table_full}. Creating sample data for demonstration.\")\n",
        "    \n",
        "    # Create sample source data\n",
        "    sample_data = [\n",
        "        (\"CUST001\", \"John\", \"Doe\", \"john.doe@example.com\", \"2024-01-15\", \"active\"),\n",
        "        (\"CUST002\", \"Jane\", \"Smith\", \"jane.smith@example.com\", \"2024-01-16\", \"active\"),\n",
        "        (\"CUST003\", \"Bob\", \"Johnson\", \"bob.johnson@example.com\", \"2024-01-17\", \"inactive\"),\n",
        "        (\"CUST004\", \"Alice\", \"Williams\", \"alice.williams@example.com\", \"2024-01-18\", \"active\"),\n",
        "        (\"CUST005\", \"Charlie\", \"Brown\", \"charlie.brown@example.com\", \"2024-01-19\", \"active\")\n",
        "    ]\n",
        "    \n",
        "    schema = StructType([\n",
        "        StructField(\"customer_id\", StringType(), True),\n",
        "        StructField(\"first_name\", StringType(), True),\n",
        "        StructField(\"last_name\", StringType(), True),\n",
        "        StructField(\"email\", StringType(), True),\n",
        "        StructField(\"registration_date\", StringType(), True),\n",
        "        StructField(\"status\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    df = spark.createDataFrame(sample_data, schema)\n",
        "    \n",
        "    # Create the source table for demonstration purposes\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(source_table_full)\n",
        "    record_count = df.count()\n",
        "    logger.info(f\"Created sample source table with {record_count} records\")\n",
        "    df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data: add metadata columns and apply business logic\n",
        "df_transformed = df.withColumn(\"ingestion_timestamp\", F.current_timestamp()) \\\n",
        "                   .withColumn(\"source_id\", F.lit(source_id)) \\\n",
        "                   .withColumn(\"full_name\", F.concat(F.col(\"first_name\"), F.lit(\" \"), F.col(\"last_name\"))) \\\n",
        "                   .filter(F.col(\"status\") == \"active\")  # Example: filter only active customers\n",
        "\n",
        "record_count_transformed = df_transformed.count()\n",
        "logger.info(f\"Transformed data: {record_count_transformed} records (filtered from {record_count} source records)\")\n",
        "logger.info(\"Sample transformed data:\")\n",
        "df_transformed.select(\"customer_id\", \"full_name\", \"email\", \"status\", \"source_id\", \"ingestion_timestamp\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write to Target\n",
        "\n",
        "Write transformed data to the target Delta table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_table_full = f\"{catalog}.{schema}.{target_table}\"\n",
        "\n",
        "logger.info(f\"Writing to target table: {target_table_full}\")\n",
        "logger.info(f\"Write mode: {write_mode}\")\n",
        "logger.info(f\"Records to write: {record_count_transformed}\")\n",
        "\n",
        "# Write to target Delta table\n",
        "try:\n",
        "    df_transformed.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(write_mode) \\\n",
        "        .option(\"mergeSchema\", \"true\") \\\n",
        "        .saveAsTable(target_table_full)\n",
        "    \n",
        "    logger.info(f\"✅ Successfully wrote {record_count_transformed} records to {target_table_full}\")\n",
        "    \n",
        "    # Verify the write\n",
        "    written_df = spark.table(target_table_full)\n",
        "    written_count = written_df.count()\n",
        "    logger.info(f\"✅ Verified: {written_count} records in target table\")\n",
        "    written_df.select(\"customer_id\", \"full_name\", \"email\", \"status\", \"source_id\", \"ingestion_timestamp\").show(5, truncate=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to write to target table: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "✅ Metadata read from control table using source_id  \n",
        "✅ Configuration parsed and validated  \n",
        "✅ Source data read from Delta table (or created for demo)  \n",
        "✅ Data transformed and enriched  \n",
        "✅ Data written to target Delta table  \n",
        "\n",
        "**Framework Contract:**  \n",
        "This notebook demonstrates the expected contract:\n",
        "- Accepts `source_id` and `control_table` as inputs via widgets\n",
        "- Reads catalog, schema, and table names from control table metadata\n",
        "- Validates configuration\n",
        "- Reads from source Delta table\n",
        "- Transforms data (adds metadata columns, applies business logic)\n",
        "- Writes to target Delta table\n",
        "\n",
        "**Widgets Used:**\n",
        "- `source_id`: Unique identifier for this ingestion task (used to query control table)\n",
        "- `control_table`: Name of the control table containing job metadata\n",
        "\n",
        "**Metadata Structure:**\n",
        "The notebook reads from the control table:\n",
        "- `source_config`: JSON string with `catalog`, `schema`, `table` (source table location)\n",
        "- `target_config`: JSON string with `catalog`, `schema`, `table`, `write_mode` (target table location and write mode)\n",
        "\n",
        "**Configuration:**\n",
        "- Write mode comes from `target_config.write_mode` (defaults to `overwrite` if not specified)\n",
        "- Both source and target use the same catalog and schema (from source_config/target_config)\n",
        "\n",
        "The framework will set `source_id` and `control_table` widgets when calling this notebook as a task.\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "language": "python",
      "notebookName": "sample_ingestion_notebook"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
